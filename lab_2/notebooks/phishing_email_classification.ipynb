{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for Phishing Email Detection\n",
    "\n",
    "## –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã:\n",
    "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**\n",
    "2. **NLP Preprocessing**\n",
    "3. **EDA –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ (Word Clouds)**\n",
    "4. **–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (TF-IDF, Count)**\n",
    "5. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**\n",
    "6. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "try:\n",
    "    with open('../config/config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Config –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "except:\n",
    "    config = {'data': {'text_column': 'text', 'label_column': 'label'}}\n",
    "\n",
    "# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç\n",
    "try:\n",
    "    data = pd.read_csv('../data/phishing_emails.csv')\n",
    "    print(f\"‚úÖ Phishing dataset: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        data = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "        data = data[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})\n",
    "        data['label'] = data['label'].map({'spam': 1, 'ham': 0})\n",
    "        print(f\"‚úÖ SMS Spam dataset: {data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ...\")\n",
    "        # –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ\n",
    "        phishing = [\"URGENT click here verify account\", \"Free money claim now\", \"Security alert update info\"]\n",
    "        legitimate = [\"Hi how are you\", \"Meeting tomorrow 3PM\", \"Your order shipped\"]\n",
    "\n",
    "        texts = phishing * 50 + legitimate * 50\n",
    "        labels = [1] * 150 + [0] * 150\n",
    "        data = pd.DataFrame({'text': texts, 'label': labels})\n",
    "        print(f\"‚úÖ –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ: {data.shape}\")\n",
    "\n",
    "text_col = 'text'\n",
    "label_col = 'label'\n",
    "print(f\"–ö–ª–∞—Å—Å—ã: {data[label_col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Preprocessing\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '[URL]', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º preprocessing\n",
    "data['text_clean'] = data[text_col].apply(preprocess_text)\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\n–ò—Å—Ö–æ–¥–Ω—ã–π: {data.iloc[i][text_col]}\")\n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {data.iloc[i]['text_clean']}\")\n",
    "    print(f\"–ö–ª–∞—Å—Å: {data.iloc[i][label_col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Word Clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, label in enumerate(sorted(data[label_col].unique())):\n",
    "    texts = ' '.join(data[data[label_col] == label]['text_clean'])\n",
    "\n",
    "    if len(texts.strip()) > 0:\n",
    "        wordcloud = WordCloud(\n",
    "            width=600, height=300,\n",
    "            background_color='white',\n",
    "            max_words=50\n",
    "        ).generate(texts)\n",
    "\n",
    "        axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "        class_name = 'Legitimate' if label == 0 else 'Phishing'\n",
    "        axes[idx].set_title(f'{class_name} (Class {label})')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "data['text_length'] = data['text_clean'].str.len()\n",
    "data['word_count'] = data['text_clean'].str.split().str.len()\n",
    "\n",
    "print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:\")\n",
    "print(data.groupby(label_col)[['text_length', 'word_count']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "X = data['text_clean']\n",
    "y = data[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"Train classes: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "vectorizers = {\n",
    "    'TF-IDF': TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    ),\n",
    "    'Count': CountVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )\n",
    "}\n",
    "\n",
    "vectorized_data = {}\n",
    "for name, vec in vectorizers.items():\n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "    vectorized_data[name] = {\n",
    "        'vectorizer': vec,\n",
    "        'X_train': X_train_vec,\n",
    "        'X_test': X_test_vec\n",
    "    }\n",
    "    print(f\"{name}: {X_train_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for vec_name, vec_data in vectorized_data.items():\n",
    "    print(f\"\\n=== {vec_name} ===\")\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # –û–±—É—á–µ–Ω–∏–µ\n",
    "        model.fit(vec_data['X_train'], y_train)\n",
    "        y_pred = model.predict(vec_data['X_test'])\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'Vectorizer': vec_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "\n",
    "        print(f\"{model_name}: Acc={accuracy:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å\n",
    "best_idx = results_df['F1-Score'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "print(f\"\\nüèÜ –õ—É—á—à–∞—è: {best['Model']} + {best['Vectorizer']}\")\n",
    "print(f\"F1-Score: {best['F1-Score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "best_vec_name = best['Vectorizer']\n",
    "best_model_name = best['Model']\n",
    "\n",
    "# –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "best_vec_data = vectorized_data[best_vec_name]\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(best_vec_data['X_train'], y_train)\n",
    "y_pred_best = best_model.predict(best_vec_data['X_test'])\n",
    "\n",
    "print(f\"üîç –ê–Ω–∞–ª–∏–∑: {best_model_name} + {best_vec_name}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'])\n",
    "plt.title(f'Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)\n",
    "if hasattr(best_model, 'coef_') and best_model.coef_ is not None:\n",
    "    vectorizer = best_vec_data['vectorizer']\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = best_model.coef_[0] if len(best_model.coef_) == 1 else best_model.coef_[1]\n",
    "\n",
    "    # –¢–æ–ø phishing –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "    top_phishing_idx = coef.argsort()[-10:][::-1]\n",
    "    print(\"\\nüìà –¢–æ–ø phishing –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã:\")\n",
    "    for idx in top_phishing_idx:\n",
    "        print(f\"  {feature_names[idx]}: {coef[idx]:.3f}\")\n",
    "\n",
    "    # –¢–æ–ø legitimate –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "    top_legit_idx = coef.argsort()[:10]\n",
    "    print(\"\\nüìâ –¢–æ–ø legitimate –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã:\")\n",
    "    for idx in top_legit_idx:\n",
    "        print(f\"  {feature_names[idx]}: {coef[idx]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "...\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "...\n",
    "\n",
    "### Production:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
